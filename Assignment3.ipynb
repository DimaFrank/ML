{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e69940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1396a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reviews_data.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "121416c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oct nice trendy hotel location not too bad stayed in this hotel for one night as this is fairly new place some of the taxi drivers did not know where it was and or did not want to drive there once have eventually arrived at the hotel was very pleasantly surprised with the decor of the lobby ground floor area it was very stylish and modern found the reception staff geeting me with aloha bit out of place but guess they are briefed to say that to keep up the coroporate image as have starwood preferred guest member was given small gift upon check in it was only couple of fridge magnets in gift box but nevertheless nice gesture my room was nice and roomy there are tea and coffee facilities in each room and you get two complimentary bottles of water plus some toiletries by bliss the location is not great it is at the last metro stop and you then need to take taxi but if you are not planning on going to see the historic sites in beijing then you will be ok chose to have some breakfast in the hotel which was really tasty and there was good selection of dishes there are couple of computers to use in the communal area as well as pool table there is also small swimming pool and gym area would definitely stay in this hotel again but only if did not plan to travel to central beijing as it can take long time the location is ok if you plan to do lot of shopping as there is big shopping centre just few minutes away from the hotel and there are plenty of eating options around including restaurants that serve dog meat\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81b9ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = text[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6f0d100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer with a minimum frequency of 10k and more than 90k\n",
    "tokenizer = Tokenizer(lower=True, oov_token=\"<OOV>\", filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(sample)\n",
    "\n",
    "# Filter out words with frequency less than 10k and more than 90k\n",
    "word_counts = tokenizer.word_counts\n",
    "filtered_word_counts = {word: count for word, count in word_counts.items() if count >= 10000 and count<=90000}\n",
    "\n",
    "# Create a new tokenizer with the filtered words\n",
    "filtered_tokens = list(filtered_word_counts.keys())\n",
    "filtered_tokenizer = Tokenizer(lower=True, oov_token=\"<OOV>\", filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', num_words=len(filtered_tokens))\n",
    "filtered_tokenizer.fit_on_texts(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62770583",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = filtered_tokenizer.word_index\n",
    "# word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ea5972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word embeddings\n",
    "embedding_dim = 5\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b564fff1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m pair\n\u001b[0;32m     14\u001b[0m tokens \u001b[38;5;241m=\u001b[39m filtered_tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(sample)\n\u001b[1;32m---> 15\u001b[0m skip_grams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgenerate_skipgrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiltered_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_negative_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mgenerate_skipgrams\u001b[1;34m(texts, tokenizer, skip_window, num_negative_samples, vocab_size)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[0;32m      9\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences([text])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 10\u001b[0m     pairs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mskipgrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocabulary_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_negative_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pair \u001b[38;5;129;01min\u001b[39;00m pairs:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m pair\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\preprocessing\\sequence.py:380\u001b[0m, in \u001b[0;36mskipgrams\u001b[1;34m(sequence, vocabulary_size, window_size, negative_samples, shuffle, categorical, sampling_table, seed)\u001b[0m\n\u001b[0;32m    378\u001b[0m     seed \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10e6\u001b[39m)\n\u001b[0;32m    379\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m--> 380\u001b[0m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcouples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[0;32m    382\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(labels)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\random.py:362\u001b[0m, in \u001b[0;36mRandom.shuffle\u001b[1;34m(self, x, random)\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(x))):\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;66;03m# pick an element in x[:i+1] with which to exchange x[i]\u001b[39;00m\n\u001b[0;32m    361\u001b[0m         j \u001b[38;5;241m=\u001b[39m randbelow(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 362\u001b[0m         x[i], x[j] \u001b[38;5;241m=\u001b[39m x[j], x[i]\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    364\u001b[0m     _warn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe *random* parameter to shuffle() has been deprecated\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    365\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msince Python 3.9 and will be removed in a subsequent \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    366\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    367\u001b[0m           \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate skip-grams\n",
    "\n",
    "skip_window = 5\n",
    "num_negative_samples = 2\n",
    "skip_grams = []\n",
    "\n",
    "def generate_skipgrams(texts, tokenizer, skip_window, num_negative_samples, vocab_size):\n",
    "    for text in texts:\n",
    "        tokens = tokenizer.texts_to_sequences([text])[0]\n",
    "        pairs, _ = skipgrams(tokens, vocabulary_size=vocab_size, window_size=skip_window, negative_samples=num_negative_samples)\n",
    "        for pair in pairs:\n",
    "            yield pair\n",
    "            \n",
    "tokens = filtered_tokenizer.texts_to_sequences(sample)\n",
    "skip_grams = list(generate_skipgrams(sample, filtered_tokenizer, skip_window, num_negative_samples, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7298d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building\n",
    "\n",
    "# Define input layer\n",
    "input_word = Input(shape=(1,))\n",
    "\n",
    "# Define embedding layer\n",
    "embedding = embedding_layer(input_word)\n",
    "\n",
    "# Define flatten layer to flatten embedding output\n",
    "flatten = Flatten()(embedding)\n",
    "\n",
    "# Define output layer with softmax activation\n",
    "output_word = Dense(vocab_size, activation='softmax')(flatten)\n",
    "\n",
    "# Define skipgram model with input and output layers\n",
    "model = Model(inputs=input_word, outputs=output_word)\n",
    "\n",
    "# Compile model with categorical cross-entropy loss and Adam optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2035bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "# Convert skip-grams to input and output data\n",
    "pairs, labels = zip(*skip_grams)\n",
    "pairs = np.array(pairs, dtype=np.int32)\n",
    "labels = to_categorical(np.array(labels, dtype=np.int32), num_classes=vocab_size)\n",
    "\n",
    "# Train model for 10 epochs\n",
    "model.fit(pairs, labels, epochs=10, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8858fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c855a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb078a00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed0bd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the context words\n",
    "# x_train = []\n",
    "# y_train = []\n",
    "# for pair in skip_grams:\n",
    "#     center_word = pair[0]\n",
    "#     context_word = pair[1]\n",
    "#     x_train.append(center_word)\n",
    "#     y_train.append(to_categorical(context_word, num_classes=vocab_size))\n",
    "\n",
    "\n",
    "# batch_size = 128\n",
    "# num_epochs = 10\n",
    "# steps_per_epoch = len(skip_grams) // batch_size\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     for step in range(steps_per_epoch):\n",
    "#         batch_skip_grams = skip_grams[step*batch_size:(step+1)*batch_size]\n",
    "#         x_train = []\n",
    "#         y_train = []\n",
    "#         for pair in batch_skip_grams:\n",
    "#             center_word = pair[0]\n",
    "#             context_word = pair[1]\n",
    "#             x_train.append(center_word)\n",
    "#             y_train.append(to_categorical(context_word, num_classes=vocab_size))\n",
    "#         model.train_on_batch(x_train, y_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
